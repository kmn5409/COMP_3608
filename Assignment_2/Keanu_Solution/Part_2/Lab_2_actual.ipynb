{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient Descent is an important iterative 1st order optimization technique that was initially conceived for use in minimising convex functions without constraints.\n",
    "\n",
    "In this lab, we will look at the use of gradient descent using PyTorch. You will also get practice tracing through instances of gradient descent by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiaion\n",
    "\n",
    "There are three main ways to compute derivatives:\n",
    "1. Symbolically - by using algebraic rules to generate or derive the derivative\n",
    "2. Numerically - using calculations to approximate the derivative at a point\n",
    "3. Automatically - using the structure of code used to implement a function to derive its gradient\n",
    "\n",
    "Most machine learning techniques use automatic differentiaion to help ease implementation. Automatic differentation works by assebling a computation graph. For example, the function $f(x) = x_1 + x_2$ may be represented by the following computation graph:\n",
    "\n",
    "![Simple Computation Graph](img/compgraph1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "PyTorch is a library for tensor manipulation and automatic differentiation. There are some vital concepts from PyTorch that we will use throughout the course:\n",
    "1. tensor - the primary means of storing data for tensor manipulation in Pytorch\n",
    "2. parameter - these are \"trainable\" tensors that can gave gradients that can be passed to an optimizer\n",
    "3. model - used to represent a sequence of operations on paramters and input\n",
    "4. optimizer - an algorithm that is used to train paramters based on their gradient with respect to some loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Consider the function $f(x) = 2x^2 + 3x - 4$. Suppose that we wish to minimize this function. Our parameter here will be our $x$, and our loss is $f$. We have several way of abstracting this in accord with PyTorch's facilities and standards. What we will see is but one way of doing so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch as th\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def plot_loss_curve(loss_curve):\n",
    "    plt.plot(list(range(len(loss_curve))), loss_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model with parameter and loss function\n",
    "\n",
    "def f(x):\n",
    "    return 2 * x * x + 3 * x - 4\n",
    "    \n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        x = th.tensor(np.random.random()) # generate random point\n",
    "        self.x = nn.Parameter(x)\n",
    "    \n",
    "    def loss(self):\n",
    "        return f(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final x^* is  tensor(-0.7500)\n"
     ]
    }
   ],
   "source": [
    "model = MyModel() # model instance\n",
    "lr = 0.1 # our learning rate \n",
    "loss_curve = []\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # zero out gradients to prevent gradients from carrying over into other iterations\n",
    "    loss = model.loss() # compute loss\n",
    "    loss.backward() # compute gradient with respect to loss function\n",
    "    loss_curve.append(loss.item())\n",
    "    optimizer.step() # use gradient descent to adjust value of paramters in model\n",
    "\n",
    "print('Our final x^* is ', model.x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss curve')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(loss_curve)\n",
    "plt.xlabel('$i$')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss curve above can be ploted to show how the loss is decreasing per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable case\n",
    "As mentioned in class, gradient descent is trivially extensible to multivariable, vector, and matrix settings once the output of the loss function is real scalar value.\n",
    "\n",
    "For an example, we shall first consider function $f(x, y) = (x^2 + y^2) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, y):\n",
    "    print('x is {}'.format(x.data))\n",
    "    print('y is {}'.format(y.data))\n",
    "    print(x**2 + x*y)\n",
    "    print('\\n\\n')\n",
    "    return (x**2 + x*y)\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #x = th.tensor(np.random.random()) # generate x co-ordinate of random point\n",
    "        #y = th.tensor(np.random.random()) # generate y co-ordinate of random point\n",
    "        x = th.tensor(2.0)\n",
    "        y = th.tensor(1.0)\n",
    "        self.x = nn.Parameter(x)\n",
    "        self.y = nn.Parameter(y)\n",
    "        \n",
    "    def loss(self):\n",
    "        return loss_function(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is 2.0\n",
      "y is 1.0\n",
      "tensor(6., grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x is 5.0\n",
      "Grad at y is 2.0\n",
      "x is 1.5\n",
      "y is 0.800000011920929\n",
      "tensor(3.4500, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x is 3.799999952316284\n",
      "Grad at y is 1.5\n",
      "Our final x^* is  tensor(1.1200)\n",
      "Our final y^* is  tensor(0.6500)\n"
     ]
    }
   ],
   "source": [
    "model = MyModel() # model instance\n",
    "lr = 0.1 # our learning rate \n",
    "loss_curve = []\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # zero out gradients to prevent gradients from carrying over into other iterations\n",
    "    loss = model.loss() # compute loss\n",
    "    loss.backward() # compute gradient with respect to loss function\n",
    "    print('Grad at x is {}'.format(model.x.grad))\n",
    "    print('Grad at y is {}'.format(model.y.grad))\n",
    "    loss_curve.append(loss.item())\n",
    "    optimizer.step() # use gradient descent to adjust value of paramters in model\n",
    "\n",
    "print('Our final x^* is ', model.x.data)\n",
    "print('Our final y^* is ', model.y.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector case\n",
    "\n",
    "Not the above function could have been written as $f(x) = (x_{1}^{2} + x_{2}^{2})^{2} + 4$. Instead of separating out the paramters into seperate variables, we could have kept together as a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0] is 0.8128640125997446\n",
      "x[1] is 0.19353373623038628\n",
      "tensor(4.6982, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.6257, 0.3871], dtype=torch.float64)\n",
      "x[0] is 0.7315776113397702\n",
      "x[1] is 0.17418036260734765\n",
      "tensor(4.5655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.4632, 0.3484], dtype=torch.float64)\n",
      "x[0] is 0.6584198502057932\n",
      "x[1] is 0.15676232634661288\n",
      "tensor(4.4581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.3168, 0.3135], dtype=torch.float64)\n",
      "x[0] is 0.5925778651852138\n",
      "x[1] is 0.1410860937119516\n",
      "tensor(4.3711, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.1852, 0.2822], dtype=torch.float64)\n",
      "x[0] is 0.5333200786666924\n",
      "x[1] is 0.12697748434075645\n",
      "tensor(4.3006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.0666, 0.2540], dtype=torch.float64)\n",
      "x[0] is 0.47998807080002315\n",
      "x[1] is 0.11427973590668081\n",
      "tensor(4.2434, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.9600, 0.2286], dtype=torch.float64)\n",
      "x[0] is 0.43198926372002083\n",
      "x[1] is 0.10285176231601273\n",
      "tensor(4.1972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.8640, 0.2057], dtype=torch.float64)\n",
      "x[0] is 0.38879033734801877\n",
      "x[1] is 0.09256658608441146\n",
      "tensor(4.1597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.7776, 0.1851], dtype=torch.float64)\n",
      "x[0] is 0.3499113036132169\n",
      "x[1] is 0.08330992747597031\n",
      "tensor(4.1294, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.6998, 0.1666], dtype=torch.float64)\n",
      "x[0] is 0.3149201732518952\n",
      "x[1] is 0.07497893472837328\n",
      "tensor(4.1048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.6298, 0.1500], dtype=torch.float64)\n",
      "x[0] is 0.28342815592670567\n",
      "x[1] is 0.06748104125553595\n",
      "tensor(4.0849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.5669, 0.1350], dtype=torch.float64)\n",
      "x[0] is 0.2550853403340351\n",
      "x[1] is 0.06073293712998235\n",
      "tensor(4.0688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.5102, 0.1215], dtype=torch.float64)\n",
      "x[0] is 0.22957680630063163\n",
      "x[1] is 0.054659643416984115\n",
      "tensor(4.0557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.4592, 0.1093], dtype=torch.float64)\n",
      "x[0] is 0.20661912567056845\n",
      "x[1] is 0.049193679075285705\n",
      "tensor(4.0451, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.4132, 0.0984], dtype=torch.float64)\n",
      "x[0] is 0.1859572131035116\n",
      "x[1] is 0.04427431116775713\n",
      "tensor(4.0365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.3719, 0.0885], dtype=torch.float64)\n",
      "x[0] is 0.16736149179316043\n",
      "x[1] is 0.039846880050981415\n",
      "tensor(4.0296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.3347, 0.0797], dtype=torch.float64)\n",
      "x[0] is 0.1506253426138444\n",
      "x[1] is 0.03586219204588327\n",
      "tensor(4.0240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.3013, 0.0717], dtype=torch.float64)\n",
      "x[0] is 0.13556280835245996\n",
      "x[1] is 0.03227597284129494\n",
      "tensor(4.0194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.2711, 0.0646], dtype=torch.float64)\n",
      "x[0] is 0.12200652751721396\n",
      "x[1] is 0.02904837555716545\n",
      "tensor(4.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.2440, 0.0581], dtype=torch.float64)\n",
      "x[0] is 0.10980587476549256\n",
      "x[1] is 0.026143538001448904\n",
      "tensor(4.0127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.2196, 0.0523], dtype=torch.float64)\n",
      "x[0] is 0.0988252872889433\n",
      "x[1] is 0.023529184201304013\n",
      "tensor(4.0103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1977, 0.0471], dtype=torch.float64)\n",
      "x[0] is 0.08894275856004896\n",
      "x[1] is 0.02117626578117361\n",
      "tensor(4.0084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1779, 0.0424], dtype=torch.float64)\n",
      "x[0] is 0.08004848270404406\n",
      "x[1] is 0.01905863920305625\n",
      "tensor(4.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1601, 0.0381], dtype=torch.float64)\n",
      "x[0] is 0.07204363443363966\n",
      "x[1] is 0.017152775282750627\n",
      "tensor(4.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1441, 0.0343], dtype=torch.float64)\n",
      "x[0] is 0.0648392709902757\n",
      "x[1] is 0.015437497754475565\n",
      "tensor(4.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1297, 0.0309], dtype=torch.float64)\n",
      "x[0] is 0.05835534389124813\n",
      "x[1] is 0.013893747979028008\n",
      "tensor(4.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1167, 0.0278], dtype=torch.float64)\n",
      "x[0] is 0.052519809502123316\n",
      "x[1] is 0.012504373181125206\n",
      "tensor(4.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.1050, 0.0250], dtype=torch.float64)\n",
      "x[0] is 0.04726782855191099\n",
      "x[1] is 0.011253935863012685\n",
      "tensor(4.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0945, 0.0225], dtype=torch.float64)\n",
      "x[0] is 0.04254104569671989\n",
      "x[1] is 0.010128542276711417\n",
      "tensor(4.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0851, 0.0203], dtype=torch.float64)\n",
      "x[0] is 0.0382869411270479\n",
      "x[1] is 0.009115688049040276\n",
      "tensor(4.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0766, 0.0182], dtype=torch.float64)\n",
      "x[0] is 0.03445824701434311\n",
      "x[1] is 0.008204119244136249\n",
      "tensor(4.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0689, 0.0164], dtype=torch.float64)\n",
      "x[0] is 0.031012422312908797\n",
      "x[1] is 0.007383707319722624\n",
      "tensor(4.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0620, 0.0148], dtype=torch.float64)\n",
      "x[0] is 0.027911180081617918\n",
      "x[1] is 0.006645336587750361\n",
      "tensor(4.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0558, 0.0133], dtype=torch.float64)\n",
      "x[0] is 0.025120062073456125\n",
      "x[1] is 0.005980802928975325\n",
      "tensor(4.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0502, 0.0120], dtype=torch.float64)\n",
      "x[0] is 0.02260805586611051\n",
      "x[1] is 0.005382722636077792\n",
      "tensor(4.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0452, 0.0108], dtype=torch.float64)\n",
      "x[0] is 0.02034725027949946\n",
      "x[1] is 0.004844450372470013\n",
      "tensor(4.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0407, 0.0097], dtype=torch.float64)\n",
      "x[0] is 0.018312525251549513\n",
      "x[1] is 0.004360005335223012\n",
      "tensor(4.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0366, 0.0087], dtype=torch.float64)\n",
      "x[0] is 0.01648127272639456\n",
      "x[1] is 0.00392400480170071\n",
      "tensor(4.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0330, 0.0078], dtype=torch.float64)\n",
      "x[0] is 0.014833145453755105\n",
      "x[1] is 0.003531604321530639\n",
      "tensor(4.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0297, 0.0071], dtype=torch.float64)\n",
      "x[0] is 0.013349830908379596\n",
      "x[1] is 0.003178443889377575\n",
      "tensor(4.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0267, 0.0064], dtype=torch.float64)\n",
      "x[0] is 0.012014847817541635\n",
      "x[1] is 0.0028605995004398173\n",
      "tensor(4.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0240, 0.0057], dtype=torch.float64)\n",
      "x[0] is 0.010813363035787471\n",
      "x[1] is 0.0025745395503958355\n",
      "tensor(4.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0216, 0.0051], dtype=torch.float64)\n",
      "x[0] is 0.009732026732208724\n",
      "x[1] is 0.002317085595356252\n",
      "tensor(4.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0195, 0.0046], dtype=torch.float64)\n",
      "x[0] is 0.008758824058987851\n",
      "x[1] is 0.002085377035820627\n",
      "tensor(4.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0175, 0.0042], dtype=torch.float64)\n",
      "x[0] is 0.007882941653089066\n",
      "x[1] is 0.0018768393322385643\n",
      "tensor(4.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0158, 0.0038], dtype=torch.float64)\n",
      "x[0] is 0.00709464748778016\n",
      "x[1] is 0.001689155399014708\n",
      "tensor(4.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0142, 0.0034], dtype=torch.float64)\n",
      "x[0] is 0.006385182739002144\n",
      "x[1] is 0.0015202398591132372\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0128, 0.0030], dtype=torch.float64)\n",
      "x[0] is 0.0057466644651019295\n",
      "x[1] is 0.0013682158732019134\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0115, 0.0027], dtype=torch.float64)\n",
      "x[0] is 0.005171998018591736\n",
      "x[1] is 0.0012313942858817222\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0103, 0.0025], dtype=torch.float64)\n",
      "x[0] is 0.004654798216732562\n",
      "x[1] is 0.0011082548572935499\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0093, 0.0022], dtype=torch.float64)\n",
      "x[0] is 0.0041893183950593065\n",
      "x[1] is 0.000997429371564195\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0084, 0.0020], dtype=torch.float64)\n",
      "x[0] is 0.003770386555553376\n",
      "x[1] is 0.0008976864344077755\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0075, 0.0018], dtype=torch.float64)\n",
      "x[0] is 0.0033933478999980385\n",
      "x[1] is 0.0008079177909669979\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0068, 0.0016], dtype=torch.float64)\n",
      "x[0] is 0.0030540131099982344\n",
      "x[1] is 0.0007271260118702981\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0061, 0.0015], dtype=torch.float64)\n",
      "x[0] is 0.002748611798998411\n",
      "x[1] is 0.0006544134106832683\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0055, 0.0013], dtype=torch.float64)\n",
      "x[0] is 0.00247375061909857\n",
      "x[1] is 0.0005889720696149414\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0049, 0.0012], dtype=torch.float64)\n",
      "x[0] is 0.0022263755571887128\n",
      "x[1] is 0.0005300748626534472\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0045, 0.0011], dtype=torch.float64)\n",
      "x[0] is 0.0020037380014698413\n",
      "x[1] is 0.0004770673763881025\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0040, 0.0010], dtype=torch.float64)\n",
      "x[0] is 0.0018033642013228573\n",
      "x[1] is 0.0004293606387492923\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0036, 0.0009], dtype=torch.float64)\n",
      "x[0] is 0.0016230277811905716\n",
      "x[1] is 0.000386424574874363\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0032, 0.0008], dtype=torch.float64)\n",
      "x[0] is 0.0014607250030715144\n",
      "x[1] is 0.0003477821173869267\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0029, 0.0007], dtype=torch.float64)\n",
      "x[0] is 0.0013146525027643628\n",
      "x[1] is 0.00031300390564823404\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0026, 0.0006], dtype=torch.float64)\n",
      "x[0] is 0.0011831872524879264\n",
      "x[1] is 0.0002817035150834106\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0024, 0.0006], dtype=torch.float64)\n",
      "x[0] is 0.0010648685272391337\n",
      "x[1] is 0.00025353316357506957\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0021, 0.0005], dtype=torch.float64)\n",
      "x[0] is 0.0009583816745152204\n",
      "x[1] is 0.0002281798472175626\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0019, 0.0005], dtype=torch.float64)\n",
      "x[0] is 0.0008625435070636983\n",
      "x[1] is 0.00020536186249580634\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0017, 0.0004], dtype=torch.float64)\n",
      "x[0] is 0.0007762891563573284\n",
      "x[1] is 0.00018482567624622572\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0016, 0.0004], dtype=torch.float64)\n",
      "x[0] is 0.0006986602407215956\n",
      "x[1] is 0.00016634310862160314\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0014, 0.0003], dtype=torch.float64)\n",
      "x[0] is 0.000628794216649436\n",
      "x[1] is 0.00014970879775944284\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0013, 0.0003], dtype=torch.float64)\n",
      "x[0] is 0.0005659147949844925\n",
      "x[1] is 0.00013473791798349855\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0011, 0.0003], dtype=torch.float64)\n",
      "x[0] is 0.0005093233154860432\n",
      "x[1] is 0.0001212641261851487\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0010, 0.0002], dtype=torch.float64)\n",
      "x[0] is 0.0004583909839374389\n",
      "x[1] is 0.00010913771356663382\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0009, 0.0002], dtype=torch.float64)\n",
      "x[0] is 0.000412551885543695\n",
      "x[1] is 9.822394220997044e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0008, 0.0002], dtype=torch.float64)\n",
      "x[0] is 0.0003712966969893255\n",
      "x[1] is 8.84015479889734e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0007, 0.0002], dtype=torch.float64)\n",
      "x[0] is 0.00033416702729039294\n",
      "x[1] is 7.956139319007606e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0007, 0.0002], dtype=torch.float64)\n",
      "x[0] is 0.00030075032456135367\n",
      "x[1] is 7.160525387106846e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0006, 0.0001], dtype=torch.float64)\n",
      "x[0] is 0.0002706752921052183\n",
      "x[1] is 6.444472848396161e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0005, 0.0001], dtype=torch.float64)\n",
      "x[0] is 0.00024360776289469646\n",
      "x[1] is 5.800025563556545e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0005, 0.0001], dtype=torch.float64)\n",
      "x[0] is 0.0002192469866052268\n",
      "x[1] is 5.2200230072008904e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([0.0004, 0.0001], dtype=torch.float64)\n",
      "x[0] is 0.00019732228794470412\n",
      "x[1] is 4.6980207064808016e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([3.9464e-04, 9.3960e-05], dtype=torch.float64)\n",
      "x[0] is 0.00017759005915023371\n",
      "x[1] is 4.2282186358327214e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([3.5518e-04, 8.4564e-05], dtype=torch.float64)\n",
      "x[0] is 0.00015983105323521033\n",
      "x[1] is 3.8053967722494494e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([3.1966e-04, 7.6108e-05], dtype=torch.float64)\n",
      "x[0] is 0.0001438479479116893\n",
      "x[1] is 3.4248570950245044e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([2.8770e-04, 6.8497e-05], dtype=torch.float64)\n",
      "x[0] is 0.00012946315312052038\n",
      "x[1] is 3.082371385522054e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([2.5893e-04, 6.1647e-05], dtype=torch.float64)\n",
      "x[0] is 0.00011651683780846833\n",
      "x[1] is 2.7741342469698485e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([2.3303e-04, 5.5483e-05], dtype=torch.float64)\n",
      "x[0] is 0.0001048651540276215\n",
      "x[1] is 2.4967208222728637e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([2.0973e-04, 4.9934e-05], dtype=torch.float64)\n",
      "x[0] is 9.437863862485934e-05\n",
      "x[1] is 2.2470487400455773e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.8876e-04, 4.4941e-05], dtype=torch.float64)\n",
      "x[0] is 8.494077476237341e-05\n",
      "x[1] is 2.0223438660410197e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.6988e-04, 4.0447e-05], dtype=torch.float64)\n",
      "x[0] is 7.644669728613606e-05\n",
      "x[1] is 1.8201094794369176e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.5289e-04, 3.6402e-05], dtype=torch.float64)\n",
      "x[0] is 6.880202755752246e-05\n",
      "x[1] is 1.6380985314932257e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.3760e-04, 3.2762e-05], dtype=torch.float64)\n",
      "x[0] is 6.192182480177021e-05\n",
      "x[1] is 1.4742886783439032e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.2384e-04, 2.9486e-05], dtype=torch.float64)\n",
      "x[0] is 5.5729642321593186e-05\n",
      "x[1] is 1.3268598105095128e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.1146e-04, 2.6537e-05], dtype=torch.float64)\n",
      "x[0] is 5.0156678089433866e-05\n",
      "x[1] is 1.1941738294585615e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([1.0031e-04, 2.3883e-05], dtype=torch.float64)\n",
      "x[0] is 4.5141010280490476e-05\n",
      "x[1] is 1.0747564465127054e-05\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([9.0282e-05, 2.1495e-05], dtype=torch.float64)\n",
      "x[0] is 4.0626909252441426e-05\n",
      "x[1] is 9.672808018614349e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([8.1254e-05, 1.9346e-05], dtype=torch.float64)\n",
      "x[0] is 3.656421832719728e-05\n",
      "x[1] is 8.705527216752914e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([7.3128e-05, 1.7411e-05], dtype=torch.float64)\n",
      "x[0] is 3.290779649447755e-05\n",
      "x[1] is 7.834974495077623e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([6.5816e-05, 1.5670e-05], dtype=torch.float64)\n",
      "x[0] is 2.9617016845029797e-05\n",
      "x[1] is 7.051477045569861e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([5.9234e-05, 1.4103e-05], dtype=torch.float64)\n",
      "x[0] is 2.6655315160526818e-05\n",
      "x[1] is 6.346329341012874e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([5.3311e-05, 1.2693e-05], dtype=torch.float64)\n",
      "x[0] is 2.3989783644474134e-05\n",
      "x[1] is 5.711696406911587e-06\n",
      "tensor(4.0000, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "\n",
      "Grad at x[0] is tensor([4.7980e-05, 1.1423e-05], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def loss_function(x):\n",
    "    print('x[0] is {}'.format(x[0].data))\n",
    "    print('x[1] is {}'.format(x[1].data))\n",
    "    print((x[0]**2 + x[1]**2)+4 )\n",
    "    print('\\n\\n')\n",
    "    return (x[0] ** 2 + x[1] ** 2) + 4\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        x = th.tensor(np.random.random(2)) # generate random point\n",
    "        self.x = nn.Parameter(x)\n",
    "        \n",
    "    def loss(self):\n",
    "        return loss_function(self.x)\n",
    "    \n",
    "model = MyModel() # model instance\n",
    "lr = 0.05 # our learning rate \n",
    "loss_curve = []\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad() # zero out gradients to prevent gradients from carrying over into other iterations\n",
    "    loss = model.loss() # compute loss\n",
    "    loss.backward() # compute gradient with respect to loss function\n",
    "    loss_curve.append(loss.item())\n",
    "    print('Grad at x[0] is {}'.format(model.x.grad))\n",
    "    optimizer.step() # use gradient descent to adjust value of paramters in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(loss_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final x^* is  tensor([1.1910e-05, 1.2126e-05], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print('Our final x^* is ', model.x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "1. Write code to **maximize** the function $f(x) = -(x_{1}^{2} + x_{2}^{2} + x_{1}x_{2})^{2} - 4$. Hint recall that gradient descent minimizes functions. Recall from the lecture how we can transform minimization problems into maximization problems and vice versa.\n",
    "2. Trace through the following instances of gradient descent by hand for 2 epochs:\n",
    "    1. minimizing $f(x) =  x^3 - 2x + 4$, $x \\in \\mathbb{R}$ where $\\alpha = 0.1$, and you start at $x = -4$\n",
    "    2. minimizing $f(x) = (2-x_1)^{2}+10(x_2-x_{1}^{2})^{2}$, $x \\in \\mathbb{R}^{2}$, $\\alpha = 0.1$, and you start at $x = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear system of equations as the following:\n",
    "\n",
    "$$2x_1 + x_2 = 5$$\n",
    "$$-x_1 + x_2 = 2$$\n",
    "\n",
    "This can be re-written using matrices as the following:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}2 & 1 \\\\ -1 & 1 \\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix}5 \\\\ 2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In fact, in general, any system of linear equations such as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{11}x_1 & + & a_{12}x_2 & + & \\ldots & + & a_{1n}x_n & = & b_1 \\\\ \n",
    "a_{21}x_1 & + & a_{22}x_2 & + & \\ldots & + & a_{2n}x_n &  =  & b_2 \\\\ \n",
    "\\vdots & + & \\vdots & + & \\ldots & + & \\vdots &  =  & \\vdots \\\\ \n",
    "a_{m1}x_1 & + & a_{m2}x_2 & + & \\ldots & + & a_{mn}x_n &  =  & b_m \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "in matrix form as $Ax = b$ where \n",
    "\n",
    "$$A = \\begin{bmatrix}a_{11} & a_{12} & \\ldots & a_{1n} \\\\ \n",
    "a_{21} & a_{22} & \\ldots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ldots & \\vdots \\\\\n",
    "a_m1 & a_{m2} & \\ldots & a_{mn}  \n",
    "\\end{bmatrix}, x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}, \\text{ and } b=\\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\\end{bmatrix}$$\n",
    "\n",
    "There are several methods that we can use to find $x$. One way is to frame the problem of find $x$ as an optimization problem! We want to find $x$ such that $||Ax - b||_{2}^{2}$ is 0 (We take Euclidean distance to ensure that the difference is always non-negative, and we square to penalize high deviations more severely). Since $||Ax - b||_{2}^{2}$ is always non-negative, the smallest value it can take is 0. So minimizing means we want to bring it as close to 0 as possible. Note that this gives us an approximate answer, which for large systems or for inconsistent systems where there is no actual solution is sufficient in many cases.\n",
    "\n",
    "Hence, solving for $x$ in $Ax = b$ becomes \n",
    "$$\\min_{x} ||Ax - b||_{2}^{2}$$\n",
    "$$\\text{subject to}$$\n",
    "$$x \\in \\mathbb{R^{n}}$$\n",
    "\n",
    "There is a closed-form solution that can be solved algebraically, but we can use gradient descent to solve it as well. Problems of this form are called Least Squares Problems. As an excercise, fill in the following code for least square estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = th.tensor([[2, 1], [-1, 1]], dtype=th.float32)\n",
    "b = th.tensor([5, 2], dtype=th.float32)\n",
    "\n",
    "def loss_function(A, x, b):\n",
    "    return None # fill out loss function calculation here HINT: use th.norm\n",
    "   \n",
    "\n",
    "class LeastSquaresContainer(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        x = th.tensor(np.random.random(n), dtype=th.float32)\n",
    "        self.x = nn.Parameter(x)\n",
    "    \n",
    "    def loss(self, A, b):\n",
    "        return loss_function(A, self.x, b)\n",
    "\n",
    "\n",
    "def least_squares_approx(A, b, lr=0.01, epochs=200):\n",
    "    m, n = A.shape\n",
    "    estimator = LeastSquaresContainer(n=2)\n",
    "    optimizer = optim.SGD(estimator.parameters(), lr=0.01)\n",
    "    # FILL IN TRAINING LOOP HERE\n",
    "    return estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = least_squares_approx(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.0006, 2.9981], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.x # should get something close to [1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
