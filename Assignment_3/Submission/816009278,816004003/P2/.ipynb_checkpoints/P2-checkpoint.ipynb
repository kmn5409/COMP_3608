{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import sklearn.linear_model as linear\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as selection\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.datasets as datasets\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoonModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_func = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return None \n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        return self.loss_func(probs, y)\n",
    "    \n",
    "    def predict_proba(self, X, as_numpy=False):\n",
    "        res = self.forward(X)\n",
    "        if as_numpy:\n",
    "            res = res.detach().numpy()\n",
    "        return res\n",
    "    \n",
    "    def predict(self, X, threshold=0.5, as_numpy=False):\n",
    "        probs = self.predict_proba(X, as_numpy)\n",
    "        return probs > threshold\n",
    "    \n",
    "    def fit(self, X, y, epochs=1000, lr=0.1, lam=0):\n",
    "        optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        loss_curve = []\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss_val = self.loss(X, y) + self.regularize(lam)\n",
    "            loss_curve.append(loss_val.data.item())\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "        return loss_curve\n",
    "    \n",
    "    def regularize(self, lam):\n",
    "        loss_val = 0\n",
    "        for p in self.parameters():\n",
    "            loss_val += lam * th.norm(p)\n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN1Model(MoonModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(30, 2, bias=True)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.l2 = nn.Linear(2, 1, bias=True)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        res = self.l1(X)\n",
    "        res = self.act1(res)\n",
    "        res = self.l2(res)\n",
    "        res = self.act2(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_curve):\n",
    "    plt.plot(list(range(len(loss_curve))), loss_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wdbc.data', header=None)\n",
    "\n",
    "for i in df:\n",
    "    df[i]=df[i].replace('?', np.nan)\n",
    "    df[i].fillna(df[i].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2      3       4       5        6        7         8         9   \\\n",
      "0    17.990  10.38  122.80  1001.0  0.11840  0.27760  0.300100  0.147100   \n",
      "1    20.570  17.77  132.90  1326.0  0.08474  0.07864  0.086900  0.070170   \n",
      "2    19.690  21.25  130.00  1203.0  0.10960  0.15990  0.197400  0.127900   \n",
      "3    11.420  20.38   77.58   386.1  0.14250  0.28390  0.241400  0.105200   \n",
      "4    20.290  14.34  135.10  1297.0  0.10030  0.13280  0.198000  0.104300   \n",
      "5    12.450  15.70   82.57   477.1  0.12780  0.17000  0.157800  0.080890   \n",
      "6    18.250  19.98  119.60  1040.0  0.09463  0.10900  0.112700  0.074000   \n",
      "7    13.710  20.83   90.20   577.9  0.11890  0.16450  0.093660  0.059850   \n",
      "8    13.000  21.82   87.50   519.8  0.12730  0.19320  0.185900  0.093530   \n",
      "9    12.460  24.04   83.97   475.9  0.11860  0.23960  0.227300  0.085430   \n",
      "10   16.020  23.24  102.70   797.8  0.08206  0.06669  0.032990  0.033230   \n",
      "11   15.780  17.89  103.60   781.0  0.09710  0.12920  0.099540  0.066060   \n",
      "12   19.170  24.80  132.40  1123.0  0.09740  0.24580  0.206500  0.111800   \n",
      "13   15.850  23.95  103.70   782.7  0.08401  0.10020  0.099380  0.053640   \n",
      "14   13.730  22.61   93.60   578.3  0.11310  0.22930  0.212800  0.080250   \n",
      "15   14.540  27.54   96.73   658.8  0.11390  0.15950  0.163900  0.073640   \n",
      "16   14.680  20.13   94.74   684.5  0.09867  0.07200  0.073950  0.052590   \n",
      "17   16.130  20.68  108.10   798.8  0.11700  0.20220  0.172200  0.102800   \n",
      "18   19.810  22.15  130.00  1260.0  0.09831  0.10270  0.147900  0.094980   \n",
      "19   13.540  14.36   87.46   566.3  0.09779  0.08129  0.066640  0.047810   \n",
      "20   13.080  15.71   85.63   520.0  0.10750  0.12700  0.045680  0.031100   \n",
      "21    9.504  12.44   60.34   273.9  0.10240  0.06492  0.029560  0.020760   \n",
      "22   15.340  14.26  102.50   704.4  0.10730  0.21350  0.207700  0.097560   \n",
      "23   21.160  23.04  137.20  1404.0  0.09428  0.10220  0.109700  0.086320   \n",
      "24   16.650  21.38  110.00   904.6  0.11210  0.14570  0.152500  0.091700   \n",
      "25   17.140  16.40  116.00   912.7  0.11860  0.22760  0.222900  0.140100   \n",
      "26   14.580  21.53   97.41   644.8  0.10540  0.18680  0.142500  0.087830   \n",
      "27   18.610  20.25  122.10  1094.0  0.09440  0.10660  0.149000  0.077310   \n",
      "28   15.300  25.27  102.40   732.4  0.10820  0.16970  0.168300  0.087510   \n",
      "29   17.570  15.05  115.00   955.1  0.09847  0.11570  0.098750  0.079530   \n",
      "..      ...    ...     ...     ...      ...      ...       ...       ...   \n",
      "539   7.691  25.44   48.34   170.4  0.08668  0.11990  0.092520  0.013640   \n",
      "540  11.540  14.44   74.65   402.9  0.09984  0.11200  0.067370  0.025940   \n",
      "541  14.470  24.99   95.81   656.4  0.08837  0.12300  0.100900  0.038900   \n",
      "542  14.740  25.42   94.70   668.6  0.08275  0.07214  0.041050  0.030270   \n",
      "543  13.210  28.06   84.88   538.4  0.08671  0.06877  0.029870  0.032750   \n",
      "544  13.870  20.70   89.77   584.8  0.09578  0.10180  0.036880  0.023690   \n",
      "545  13.620  23.23   87.19   573.2  0.09246  0.06747  0.029740  0.024430   \n",
      "546  10.320  16.35   65.31   324.9  0.09434  0.04994  0.010120  0.005495   \n",
      "547  10.260  16.58   65.85   320.8  0.08877  0.08066  0.043580  0.024380   \n",
      "548   9.683  19.34   61.05   285.7  0.08491  0.05030  0.023370  0.009615   \n",
      "549  10.820  24.21   68.89   361.6  0.08192  0.06602  0.015480  0.008160   \n",
      "550  10.860  21.48   68.51   360.5  0.07431  0.04227  0.000000  0.000000   \n",
      "551  11.130  22.44   71.49   378.4  0.09566  0.08194  0.048240  0.022570   \n",
      "552  12.770  29.43   81.35   507.9  0.08276  0.04234  0.019970  0.014990   \n",
      "553   9.333  21.94   59.01   264.0  0.09240  0.05605  0.039960  0.012820   \n",
      "554  12.880  28.92   82.50   514.3  0.08123  0.05824  0.061950  0.023430   \n",
      "555  10.290  27.61   65.67   321.4  0.09030  0.07658  0.059990  0.027380   \n",
      "556  10.160  19.59   64.73   311.7  0.10030  0.07504  0.005025  0.011160   \n",
      "557   9.423  27.88   59.26   271.3  0.08123  0.04971  0.000000  0.000000   \n",
      "558  14.590  22.68   96.39   657.1  0.08473  0.13300  0.102900  0.037360   \n",
      "559  11.510  23.93   74.52   403.5  0.09261  0.10210  0.111200  0.041050   \n",
      "560  14.050  27.15   91.38   600.4  0.09929  0.11260  0.044620  0.043040   \n",
      "561  11.200  29.37   70.67   386.0  0.07449  0.03558  0.000000  0.000000   \n",
      "562  15.220  30.62  103.40   716.9  0.10480  0.20870  0.255000  0.094290   \n",
      "563  20.920  25.09  143.00  1347.0  0.10990  0.22360  0.317400  0.147400   \n",
      "564  21.560  22.39  142.00  1479.0  0.11100  0.11590  0.243900  0.138900   \n",
      "565  20.130  28.25  131.20  1261.0  0.09780  0.10340  0.144000  0.097910   \n",
      "566  16.600  28.08  108.30   858.1  0.08455  0.10230  0.092510  0.053020   \n",
      "567  20.600  29.33  140.10  1265.0  0.11780  0.27700  0.351400  0.152000   \n",
      "568   7.760  24.54   47.92   181.0  0.05263  0.04362  0.000000  0.000000   \n",
      "\n",
      "         10       11  ...      22     23      24      25       26       27  \\\n",
      "0    0.2419  0.07871  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560   \n",
      "1    0.1812  0.05667  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660   \n",
      "2    0.2069  0.05999  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450   \n",
      "3    0.2597  0.09744  ...  14.910  26.50   98.87   567.7  0.20980  0.86630   \n",
      "4    0.1809  0.05883  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500   \n",
      "5    0.2087  0.07613  ...  15.470  23.75  103.40   741.6  0.17910  0.52490   \n",
      "6    0.1794  0.05742  ...  22.880  27.66  153.20  1606.0  0.14420  0.25760   \n",
      "7    0.2196  0.07451  ...  17.060  28.14  110.60   897.0  0.16540  0.36820   \n",
      "8    0.2350  0.07389  ...  15.490  30.73  106.20   739.3  0.17030  0.54010   \n",
      "9    0.2030  0.08243  ...  15.090  40.68   97.65   711.4  0.18530  1.05800   \n",
      "10   0.1528  0.05697  ...  19.190  33.88  123.80  1150.0  0.11810  0.15510   \n",
      "11   0.1842  0.06082  ...  20.420  27.28  136.50  1299.0  0.13960  0.56090   \n",
      "12   0.2397  0.07800  ...  20.960  29.94  151.70  1332.0  0.10370  0.39030   \n",
      "13   0.1847  0.05338  ...  16.840  27.66  112.00   876.5  0.11310  0.19240   \n",
      "14   0.2069  0.07682  ...  15.030  32.01  108.80   697.7  0.16510  0.77250   \n",
      "15   0.2303  0.07077  ...  17.460  37.13  124.10   943.2  0.16780  0.65770   \n",
      "16   0.1586  0.05922  ...  19.070  30.88  123.40  1138.0  0.14640  0.18710   \n",
      "17   0.2164  0.07356  ...  20.960  31.48  136.80  1315.0  0.17890  0.42330   \n",
      "18   0.1582  0.05395  ...  27.320  30.88  186.80  2398.0  0.15120  0.31500   \n",
      "19   0.1885  0.05766  ...  15.110  19.26   99.70   711.2  0.14400  0.17730   \n",
      "20   0.1967  0.06811  ...  14.500  20.49   96.09   630.5  0.13120  0.27760   \n",
      "21   0.1815  0.06905  ...  10.230  15.66   65.13   314.9  0.13240  0.11480   \n",
      "22   0.2521  0.07032  ...  18.070  19.08  125.10   980.9  0.13900  0.59540   \n",
      "23   0.1769  0.05278  ...  29.170  35.59  188.00  2615.0  0.14010  0.26000   \n",
      "24   0.1995  0.06330  ...  26.460  31.56  177.00  2215.0  0.18050  0.35780   \n",
      "25   0.3040  0.07413  ...  22.250  21.40  152.40  1461.0  0.15450  0.39490   \n",
      "26   0.2252  0.06924  ...  17.620  33.21  122.40   896.9  0.15250  0.66430   \n",
      "27   0.1697  0.05699  ...  21.310  27.26  139.90  1403.0  0.13380  0.21170   \n",
      "28   0.1926  0.06540  ...  20.270  36.71  149.30  1269.0  0.16410  0.61100   \n",
      "29   0.1739  0.06149  ...  20.010  19.52  134.90  1227.0  0.12550  0.28120   \n",
      "..      ...      ...  ...     ...    ...     ...     ...      ...      ...   \n",
      "539  0.2037  0.07751  ...   8.678  31.89   54.49   223.6  0.15960  0.30640   \n",
      "540  0.1818  0.06782  ...  12.260  19.68   78.78   457.8  0.13450  0.21180   \n",
      "541  0.1872  0.06341  ...  16.220  31.73  113.50   808.9  0.13400  0.42020   \n",
      "542  0.1840  0.05680  ...  16.510  32.29  107.40   826.4  0.10600  0.13760   \n",
      "543  0.1628  0.05781  ...  14.370  37.17   92.48   629.6  0.10720  0.13810   \n",
      "544  0.1620  0.06688  ...  15.050  24.75   99.17   688.6  0.12640  0.20370   \n",
      "545  0.1664  0.05801  ...  15.350  29.09   97.58   729.8  0.12160  0.15170   \n",
      "546  0.1885  0.06201  ...  11.250  21.77   71.12   384.9  0.12850  0.08842   \n",
      "547  0.1669  0.06714  ...  10.830  22.04   71.08   357.4  0.14610  0.22460   \n",
      "548  0.1580  0.06235  ...  10.930  25.59   69.10   364.2  0.11990  0.09546   \n",
      "549  0.1976  0.06328  ...  13.030  31.45   83.90   505.6  0.12040  0.16330   \n",
      "550  0.1661  0.05948  ...  11.660  24.77   74.08   412.3  0.10010  0.07348   \n",
      "551  0.2030  0.06552  ...  12.020  28.26   77.80   436.6  0.10870  0.17820   \n",
      "552  0.1539  0.05637  ...  13.870  36.00   88.10   594.7  0.12340  0.10640   \n",
      "553  0.1692  0.06576  ...   9.845  25.05   62.86   295.8  0.11030  0.08298   \n",
      "554  0.1566  0.05708  ...  13.890  35.74   88.84   595.7  0.12270  0.16200   \n",
      "555  0.1593  0.06127  ...  10.840  34.91   69.57   357.6  0.13840  0.17100   \n",
      "556  0.1791  0.06331  ...  10.650  22.88   67.88   347.3  0.12650  0.12000   \n",
      "557  0.1742  0.06059  ...  10.490  34.24   66.50   330.6  0.10730  0.07158   \n",
      "558  0.1454  0.06147  ...  15.480  27.27  105.90   733.5  0.10260  0.31710   \n",
      "559  0.1388  0.06570  ...  12.480  37.16   82.28   474.2  0.12980  0.25170   \n",
      "560  0.1537  0.06171  ...  15.300  33.17  100.20   706.7  0.12410  0.22640   \n",
      "561  0.1060  0.05502  ...  11.920  38.30   75.19   439.6  0.09267  0.05494   \n",
      "562  0.2128  0.07152  ...  17.520  42.79  128.70   915.0  0.14170  0.79170   \n",
      "563  0.2149  0.06879  ...  24.290  29.41  179.10  1819.0  0.14070  0.41860   \n",
      "564  0.1726  0.05623  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130   \n",
      "565  0.1752  0.05533  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220   \n",
      "566  0.1590  0.05648  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940   \n",
      "567  0.2397  0.07016  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810   \n",
      "568  0.1587  0.05884  ...   9.456  30.37   59.16   268.6  0.08996  0.06444   \n",
      "\n",
      "          28       29      30       31  \n",
      "0    0.71190  0.26540  0.4601  0.11890  \n",
      "1    0.24160  0.18600  0.2750  0.08902  \n",
      "2    0.45040  0.24300  0.3613  0.08758  \n",
      "3    0.68690  0.25750  0.6638  0.17300  \n",
      "4    0.40000  0.16250  0.2364  0.07678  \n",
      "5    0.53550  0.17410  0.3985  0.12440  \n",
      "6    0.37840  0.19320  0.3063  0.08368  \n",
      "7    0.26780  0.15560  0.3196  0.11510  \n",
      "8    0.53900  0.20600  0.4378  0.10720  \n",
      "9    1.10500  0.22100  0.4366  0.20750  \n",
      "10   0.14590  0.09975  0.2948  0.08452  \n",
      "11   0.39650  0.18100  0.3792  0.10480  \n",
      "12   0.36390  0.17670  0.3176  0.10230  \n",
      "13   0.23220  0.11190  0.2809  0.06287  \n",
      "14   0.69430  0.22080  0.3596  0.14310  \n",
      "15   0.70260  0.17120  0.4218  0.13410  \n",
      "16   0.29140  0.16090  0.3029  0.08216  \n",
      "17   0.47840  0.20730  0.3706  0.11420  \n",
      "18   0.53720  0.23880  0.2768  0.07615  \n",
      "19   0.23900  0.12880  0.2977  0.07259  \n",
      "20   0.18900  0.07283  0.3184  0.08183  \n",
      "21   0.08867  0.06227  0.2450  0.07773  \n",
      "22   0.63050  0.23930  0.4667  0.09946  \n",
      "23   0.31550  0.20090  0.2822  0.07526  \n",
      "24   0.46950  0.20950  0.3613  0.09564  \n",
      "25   0.38530  0.25500  0.4066  0.10590  \n",
      "26   0.55390  0.27010  0.4264  0.12750  \n",
      "27   0.34460  0.14900  0.2341  0.07421  \n",
      "28   0.63350  0.20240  0.4027  0.09876  \n",
      "29   0.24890  0.14560  0.2756  0.07919  \n",
      "..       ...      ...     ...      ...  \n",
      "539  0.33930  0.05000  0.2790  0.10660  \n",
      "540  0.17970  0.06918  0.2329  0.08134  \n",
      "541  0.40400  0.12050  0.3187  0.10230  \n",
      "542  0.16110  0.10950  0.2722  0.06956  \n",
      "543  0.10620  0.07958  0.2473  0.06443  \n",
      "544  0.13770  0.06845  0.2249  0.08492  \n",
      "545  0.10490  0.07174  0.2642  0.06953  \n",
      "546  0.04384  0.02381  0.2681  0.07399  \n",
      "547  0.17830  0.08333  0.2691  0.09479  \n",
      "548  0.09350  0.03846  0.2552  0.07920  \n",
      "549  0.06194  0.03264  0.3059  0.07626  \n",
      "550  0.00000  0.00000  0.2458  0.06592  \n",
      "551  0.15640  0.06413  0.3169  0.08032  \n",
      "552  0.08653  0.06498  0.2407  0.06484  \n",
      "553  0.07993  0.02564  0.2435  0.07393  \n",
      "554  0.24390  0.06493  0.2372  0.07242  \n",
      "555  0.20000  0.09127  0.2226  0.08283  \n",
      "556  0.01005  0.02232  0.2262  0.06742  \n",
      "557  0.00000  0.00000  0.2475  0.06969  \n",
      "558  0.36620  0.11050  0.2258  0.08004  \n",
      "559  0.36300  0.09653  0.2112  0.08732  \n",
      "560  0.13260  0.10480  0.2250  0.08321  \n",
      "561  0.00000  0.00000  0.1566  0.05905  \n",
      "562  1.17000  0.23560  0.4089  0.14090  \n",
      "563  0.65990  0.25420  0.2929  0.09873  \n",
      "564  0.41070  0.22160  0.2060  0.07115  \n",
      "565  0.32150  0.16280  0.2572  0.06637  \n",
      "566  0.34030  0.14180  0.2218  0.07820  \n",
      "567  0.93870  0.26500  0.4087  0.12400  \n",
      "568  0.00000  0.00000  0.2871  0.07039  \n",
      "\n",
      "[569 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "target=df[1]\n",
    "inputs=df.drop([0,1], axis=1)\n",
    "print(inputs)\n",
    "le.fit(target)\n",
    "target=le.transform(target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)\n",
    "inputs = scaler.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = selection.KFold(n_splits=5, shuffle=True)\n",
    "kf.get_n_splits(inputs)\n",
    "\n",
    "\n",
    "#logistic regression\n",
    "f1_lr=[]\n",
    "prec_lr=[]\n",
    "rec_lr=[]\n",
    "acc_lr=[]\n",
    "\n",
    "#neural network\n",
    "\n",
    "f1_nn=[]\n",
    "prec_nn=[]\n",
    "rec_nn=[]\n",
    "acc_nn=[]\n",
    "\n",
    "for train_index, test_index in kf.split(inputs):\n",
    "    X_train, X_test, y_train, y_test = inputs[train_index], inputs[test_index], target[train_index], target[test_index]\n",
    "    \n",
    "    #logistic regression\n",
    "    logreg = LogisticRegression(solver='lbfgs')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    pred = logreg.predict(X_test)\n",
    "    f1_lr.append(metrics.f1_score(y_test, pred))\n",
    "    prec_lr.append(metrics.precision_score(y_test, pred))\n",
    "    rec_lr.append(metrics.recall_score(y_test, pred))\n",
    "    acc_lr.append(metrics.accuracy_score(y_test, pred))\n",
    "    \n",
    "    X_train = th.tensor(X_train, dtype=th.float32)\n",
    "    X_test = th.tensor(X_test,dtype=th.float32)\n",
    "    y_train = th.tensor(y_train, dtype=th.float32).view(-1, 1)\n",
    "    y_test = th.tensor(y_test, dtype=th.float32).view(-1, 1)\n",
    "    \n",
    "    #neural network\n",
    "    model = NN1Model()\n",
    "    curve = model.fit(X_train, y_train, lr=0.007, lam=0.01)\n",
    "    plot_loss(curve)\n",
    "    predictions = model.predict(X_test, as_numpy=True)\n",
    "    f1_nn.append(metrics.f1_score(y_test, predictions))\n",
    "    prec_nn.append(metrics.precision_score(y_test, pred))\n",
    "    rec_nn.append(metrics.recall_score(y_test, pred))\n",
    "    acc_nn.append(metrics.accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Average F1 Score:  96.89126725381021 %\n",
      "Average Precision:  97.70408163265307 %\n",
      "Average Recall:  96.12245755830061 %\n",
      "Average Accuracy:  96.12245755830061 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print(\"Average F1 Score: \", np.mean(f1_lr)*100,\"%\")\n",
    "print(\"Average Precision: \", np.mean(prec_lr)*100,\"%\")\n",
    "print(\"Average Recall: \", np.mean(rec_lr)*100,\"%\")\n",
    "print(\"Average Accuracy: \", np.mean(rec_lr)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "Average F1 Score:  97.04414471694476 %\n",
      "Average Precision:  97.70408163265307 %\n",
      "Average Recall:  96.12245755830061 %\n",
      "Average Accuracy:  96.12245755830061 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network\")\n",
    "print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "print(\"Average F1 Score: \", np.mean(f1_nn)*100,\"%\")\n",
    "print(\"Average Precision: \", np.mean(prec_nn)*100,\"%\")\n",
    "print(\"Average Recall: \", np.mean(rec_nn)*100,\"%\")\n",
    "print(\"Average Accuracy: \", np.mean(rec_nn)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Test Result:  Ttest_relResult(statistic=-0.2834513298867198, pvalue=0.7908964414635122)\n"
     ]
    }
   ],
   "source": [
    "print(\"T Test Result: \",stats.ttest_rel(f1_lr, f1_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of precision, recall and accuracy, logistic regression and the neural network model performed almost equally.  In terms of F1, which considers both the precision and recall of the respective models, the Neural Network performed better.  However, the difference in performance of the two models is not significant.  With a p value above 0.1, we cannot reject that the models performed the same.  There is no statistically significant difference between the two models.  This is because with only one hidden layer, the neural network is not very complex and behaves much like the logistic regression classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
